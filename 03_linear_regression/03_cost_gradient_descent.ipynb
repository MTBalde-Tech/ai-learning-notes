{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014ba306",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“‰ Notebook 3 â€“ Cost Function and Gradient Descent (Linear Regression)\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Create a simple dataset  \n",
    "- Define a **linear model**  \n",
    "- Implement a **cost function**  \n",
    "- Implement **gradient descent**  \n",
    "- See how parameters are updated to fit the data  \n",
    "\n",
    "This is a core idea in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb89fa",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Simple Dataset\n",
    "\n",
    "We start with a small synthetic dataset that roughly follows a linear pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple dataset: x (input), y (target)\n",
    "x = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y = np.array([2, 4, 6, 8, 10], dtype=float)  # approximately y = 2x\n",
    "\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Simple Dataset\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c060b4",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Linear Model\n",
    "\n",
    "We use a simple linear model:\n",
    "\n",
    "\\[ \\hat{y} = wx + b \\]\n",
    "\n",
    "Where:\n",
    "- \\( w \\) is the weight (slope)  \n",
    "- \\( b \\) is the bias (intercept)  \n",
    "- \\( \\hat{y} \\) is the prediction  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68105f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x, w, b):\n",
    "    return w * x + b\n",
    "\n",
    "# Example with w = 0, b = 0\n",
    "w_test = 0.0\n",
    "b_test = 0.0\n",
    "y_pred_example = predict(x, w_test, b_test)\n",
    "print(\"Predictions with w=0, b=0:\", y_pred_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136bf958",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Cost Function (Mean Squared Error)\n",
    "\n",
    "To measure how good our model is, we use a **cost function**:\n",
    "\n",
    "\\[ J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 \\]\n",
    "\n",
    "Where:\n",
    "- \\( m \\) is the number of training examples  \n",
    "- \\( \\hat{y}^{(i)} \\) is the prediction for example i  \n",
    "- \\( y^{(i)} \\) is the true value  \n",
    "\n",
    "The goal is to **minimize** this cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59eca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = len(x)\n",
    "    predictions = predict(x, w, b)\n",
    "    errors = predictions - y\n",
    "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    return cost\n",
    "\n",
    "# Cost with w=0, b=0\n",
    "cost_initial = compute_cost(x, y, 0.0, 0.0)\n",
    "print(\"Cost with w=0, b=0:\", cost_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51424d",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Gradients of the Cost Function\n",
    "\n",
    "To minimize the cost, we need its derivatives (gradients) with respect to w and b.\n",
    "\n",
    "For the cost function:\n",
    "\n",
    "\\[ J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)})^2 \\]\n",
    "\n",
    "The gradients are:\n",
    "\n",
    "\\[ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)}) x^{(i)} \\]\n",
    "\n",
    "\\[ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (wx^{(i)} + b - y^{(i)}) \\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c0359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradients(x, y, w, b):\n",
    "    m = len(x)\n",
    "    predictions = predict(x, w, b)\n",
    "    errors = predictions - y\n",
    "    dw = (1 / m) * np.sum(errors * x)\n",
    "    db = (1 / m) * np.sum(errors)\n",
    "    return dw, db\n",
    "\n",
    "dw_initial, db_initial = compute_gradients(x, y, 0.0, 0.0)\n",
    "print(\"Initial dw:\", dw_initial)\n",
    "print(\"Initial db:\", db_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce6631",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Gradient Descent Algorithm\n",
    "\n",
    "Gradient descent updates the parameters in the opposite direction of the gradient:\n",
    "\n",
    "\\[ w := w - \\alpha \\frac{\\partial J}{\\partial w} \\]  \n",
    "\\[ b := b - \\alpha \\frac{\\partial J}{\\partial b} \\]\n",
    "\n",
    "Where \\( \\alpha \\) is the **learning rate**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(x, y, w_init, b_init, learning_rate, num_iterations):\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dw, db = compute_gradients(x, y, w, b)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        cost = compute_cost(x, y, w, b)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i:3d}: w = {w:.4f}, b = {b:.4f}, cost = {cost:.4f}\")\n",
    "\n",
    "    return w, b, cost_history\n",
    "\n",
    "w_init = 0.0\n",
    "b_init = 0.0\n",
    "learning_rate = 0.01\n",
    "num_iterations = 201\n",
    "\n",
    "w_final, b_final, cost_history = gradient_descent(x, y, w_init, b_init, learning_rate, num_iterations)\n",
    "\n",
    "print(\"\\nFinal parameters:\")\n",
    "print(\"w =\", w_final)\n",
    "print(\"b =\", b_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cefdd3",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Cost Over Iterations\n",
    "\n",
    "Let's visualize how the cost decreases during gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterations = np.arange(len(cost_history))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iterations, cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost J(w, b)\")\n",
    "plt.title(\"Cost Decrease Over Iterations\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c17e6",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Final Fitted Line\n",
    "\n",
    "Now we plot the data and the line defined by the learned parameters \\( w \\) and \\( b \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "x_line = np.linspace(min(x), max(x), 100)\n",
    "y_line = predict(x_line, w_final, b_final)\n",
    "plt.plot(x_line, y_line, label=\"Fitted line\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Linear Regression Fit with Gradient Descent\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14b066",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Summary\n",
    "\n",
    "In this notebook, you:\n",
    "\n",
    "- Created a simple linear dataset  \n",
    "- Defined a linear prediction function  \n",
    "- Implemented a cost function (mean squared error)  \n",
    "- Computed gradients of the cost  \n",
    "- Implemented gradient descent  \n",
    "- Observed the cost decreasing over iterations  \n",
    "- Plotted the final fitted line  \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
